<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="John Mount, Nina Zumel" />

<meta name="date" content="2015-10-07" />

<title>vtreat overfit</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0A%7D%0Apre%20%7B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>



<div id="header">
<h1 class="title">vtreat overfit</h1>
<h4 class="author"><em>John Mount, Nina Zumel</em></h4>
<h4 class="date"><em>2015-10-07</em></h4>
</div>


<p>Example showing safe “best practice” use of the <a href="https://cran.r-project.org/package=vtreat">‘vtreat’</a> variable preparation library. For more on vtreat see <a href="http://www.win-vector.com/blog/2015/09/vtreat-up-on-cran/">here</a> and <a href="https://github.com/WinVector/vtreat">here</a>.</p>
<p>Build an example data frame with no relation between x and y. We are using a synthetic data set so we know what the “right answer is” (no signal). False fitting on no-signal variables is bad for at least two reasons:</p>
<ul>
<li>It creates the false impression you have a good result (which you may fail to falsify).</li>
<li>Complex bad variables can starve out simple weak good variables.</li>
</ul>
<p>This example shows things we don’t want to happen, and then the additional precautions that help prevent them.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">22626</span>)
d &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span><span class="kw">sample</span>(<span class="kw">paste</span>(<span class="st">'level'</span>,<span class="dv">1</span>:<span class="dv">1000</span>,<span class="dt">sep=</span><span class="st">''</span>),<span class="dv">2000</span>,<span class="dt">replace=</span><span class="ot">TRUE</span>)) <span class="co"># independent variables.</span>
d$y &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="kw">nrow</span>(d))&gt;<span class="fl">0.5</span>  <span class="co"># the quantity to be predicted, notice: independent of variables.</span>
d$rgroup &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="dv">100</span>*<span class="kw">runif</span>(<span class="kw">nrow</span>(d)))  <span class="co"># the random group used for splitting the data set, not a variable.</span></code></pre></div>
<p>Bad practice: use same set of data to prepare variable encoding and train a model. Leads to false belief (derived from training set) that model had a good fit. Largely due to the treated variable appearing to consume only one degree of freedom, when it consumes many more. In many cases a reasonable setting of ‘pruneSig’ (say 0.01) will help against a variable being considered desirable, but selected variables may still be mis-used by downstream modeling.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dTrain &lt;-<span class="st"> </span>d[d$rgroup&lt;=<span class="dv">80</span>,,drop=<span class="ot">FALSE</span>]
dTest &lt;-<span class="st"> </span>d[d$rgroup&gt;<span class="dv">80</span>,,drop=<span class="ot">FALSE</span>]
<span class="kw">library</span>(<span class="st">'vtreat'</span>)
treatments &lt;-<span class="st"> </span>vtreat::<span class="kw">designTreatmentsC</span>(dTrain,<span class="st">'x'</span>,<span class="st">'y'</span>,<span class="ot">TRUE</span>,
  <span class="dt">rareCount=</span><span class="dv">0</span> <span class="co"># Note: usually want rareCount&gt;0, setting to zero to illustrate problem</span>
)</code></pre></div>
<pre><code>## [1] &quot;desigining treatments Wed Oct  7 10:14:49 2015&quot;
## [1] &quot;design var x Wed Oct  7 10:14:49 2015&quot;
## [1] &quot;scoring treatments Wed Oct  7 10:14:52 2015&quot;
## [1] &quot;have treatment plan Wed Oct  7 10:14:54 2015&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dTrainTreated &lt;-<span class="st"> </span>vtreat::<span class="kw">prepare</span>(treatments,dTrain,
  <span class="dt">pruneSig=</span><span class="kw">c</span>() <span class="co"># Note: usually want pruneSig to be a small fraction, setting to null to illustrate problem</span>
)
m1 &lt;-<span class="st"> </span><span class="kw">glm</span>(y~x_catB,<span class="dt">data=</span>dTrainTreated,<span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">'logit'</span>))
<span class="kw">print</span>(<span class="kw">summary</span>(m1))  <span class="co"># notice low residual deviance</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ x_catB, family = binomial(link = &quot;logit&quot;), 
##     data = dTrainTreated)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2929  -1.1909   0.2118   1.1640   3.1962  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.03167    0.05778   0.548    0.584    
## x_catB       5.52393    0.65016   8.496   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2270.5  on 1637  degrees of freedom
## Residual deviance: 1710.4  on 1636  degrees of freedom
## AIC: 1714.4
## 
## Number of Fisher Scoring iterations: 9</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dTrain$predM1 &lt;-<span class="st"> </span><span class="kw">predict</span>(m1,<span class="dt">newdata=</span>dTrainTreated,<span class="dt">type=</span><span class="st">'response'</span>)

<span class="co"># devtools::install_github(&quot;WinVector/WVPlots&quot;)</span>
<span class="co"># library('WVPlots')</span>
plotRes &lt;-<span class="st"> </span>function(d,predName,yName,title) {
  <span class="kw">print</span>(title)
  tab &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">truth=</span>d[[yName]],<span class="dt">pred=</span>d[[predName]]&gt;<span class="fl">0.5</span>)
  <span class="kw">print</span>(tab)
  diag &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="kw">vapply</span>(<span class="kw">seq_len</span>(<span class="kw">min</span>(<span class="kw">dim</span>(tab))),
                     function(i) tab[i,i],<span class="kw">numeric</span>(<span class="dv">1</span>)))
  acc &lt;-<span class="st"> </span>diag/<span class="kw">sum</span>(tab)
<span class="co">#  if(requireNamespace(&quot;WVPlots&quot;,quietly=TRUE)) {</span>
<span class="co">#     print(WVPlots::ROCPlot(d,predName,yName,title))</span>
<span class="co">#  }</span>
  <span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">'accuracy'</span>,acc))
}

<span class="kw">plotRes</span>(dTrain,<span class="st">'predM1'</span>,<span class="st">'y'</span>,<span class="st">'model1 on train'</span>)</code></pre></div>
<pre><code>## [1] &quot;model1 on train&quot;
##        pred
## truth   FALSE TRUE
##   FALSE   211  597
##   TRUE      3  827
## [1] &quot;accuracy 0.633699633699634&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dTestTreated &lt;-<span class="st"> </span>vtreat::<span class="kw">prepare</span>(treatments,dTest,<span class="dt">pruneSig=</span><span class="kw">c</span>())
dTest$predM1 &lt;-<span class="st"> </span><span class="kw">predict</span>(m1,<span class="dt">newdata=</span>dTestTreated,<span class="dt">type=</span><span class="st">'response'</span>)
<span class="kw">plotRes</span>(dTest,<span class="st">'predM1'</span>,<span class="st">'y'</span>,<span class="st">'model1 on test'</span>)</code></pre></div>
<pre><code>## [1] &quot;model1 on test&quot;
##        pred
## truth   FALSE TRUE
##   FALSE    13  162
##   TRUE     18  169
## [1] &quot;accuracy 0.502762430939227&quot;</code></pre>
<p>The above is bad: we saw a “significant” model fit on training data (even though there is no relation to be found). This means the treated training data can be confusing to machine learning techniques and to the analyst. The issue is the training data is no longer exchangeable with the test data because the training data was used to build the variable encodings. One way to avoid this is to not use the training data for variable encoding construction, but instead use a third set for this task.</p>
<p>First notice vtreat did thing there was any usable signal, and did not want us to use the variables (we got them by setting ‘pruneSig=c()’). Also notice we set rareCount=0, which allows use of very rare levels (which help drive the problem).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(treatments$scoreFrame)</code></pre></div>
<pre><code>##   varName origName varMoves PRESSRsquared psig       sig catPRSquared
## 1  x_catB        x     TRUE  -0.004849319    1 0.1545474  0.003566441
##        csig
## 1 0.1545474</code></pre>
<p>‘vtreat’ thinks a signal as strong as the one seen on the derived variable ‘x_catB’ happens at least 70% of the time for a variable with that sort of distribution: even when there is no signal. But also notice the down-stream machine learning (in this case a standard logistic regression) used the variable wrong. It gave it a non-negligible coefficient (around 3) and thought it had a reliable estimate of the coefficient and significant model that almost halved deviance (when in fact it was given nothing). So any variables that do get through may have distributional issues (and misleadingly low apparent degrees of freedom).</p>
<p>The biggest contributors to this distributional issue tend to be rare levels of categorical variables. Since the individual levels are rare we have unreliable estimates for there effects, but if there are very many of them we may see quite a large effect. To help combat this we have a control called ‘rareLevels’. Any level that is observed no more than ‘rareLevels’ times during training is re-mapped to a new special level called “rare” and not allowed to directly contribute (i.e. can not generated unique indicator columns, and doesn’t have a direct effect on ‘catB’ or ‘catN’ encodings). If all the rare levels have a distinct behavior after grouping the “rare” level can capture that.</p>
<p>Another undesirable effect is over-estimating significance of derived variable fit for ‘catB’ and ‘catN’ impact coded variables. To fight this vtreat attempts to estimate out of sample or cross-validated effect significances (when it has enough data). So with enough data setting the ‘pruneSig’ parameter during prepare will help remove noise variables. One can set ‘pruneSig’ to something like 1/number-of-columns to ensure that with high probability only an constant number of truly useless variables make it to later modeling. However, the significance of a given effect size for variables that actually have some signal (i.e. non-noise variables) can still be sensitive to in/out sample scoring and the hiding of degrees of freedom that occurs when a large categorical variable (that represents a large number of degrees of freedom) is re-coded as an impact or effect (which appears to have only a single degree of freedom).</p>
<p>We next show how to avoid these undesirable illusory effects: better practice in partitioning and using training data. We are doing more with our data (essentially chaining models), so we have to take a bit more care with our data.</p>
<p>Correct example: use different sets to prepare variable encoding and train a model. Leads to false belief (derived from training set) that model had a good fit. Largely due to the treated variable appearing to consume only one degree of freedom, when it consumes many more.</p>
<p>Remember, the goal isn’t good performance on training- it is good performance on future data (simulated by test). So doing well on training and bad on test is worse than doing bad on both test and training.</p>
<p>Below is part of our suggested work pattern: coding/train/test split.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dCode &lt;-<span class="st"> </span>d[d$rgroup&lt;=<span class="dv">20</span>,,drop=<span class="ot">FALSE</span>]
dTrain &lt;-<span class="st"> </span>d[(d$rgroup&gt;<span class="dv">20</span>) &amp;<span class="st"> </span>(d$rgroup&lt;=<span class="dv">80</span>),,drop=<span class="ot">FALSE</span>]
treatments &lt;-<span class="st"> </span>vtreat::<span class="kw">designTreatmentsC</span>(dCode,<span class="st">'x'</span>,<span class="st">'y'</span>,<span class="ot">TRUE</span>,
                                        <span class="dt">rareCount=</span><span class="dv">0</span>  <span class="co"># Note set this to something larger, like 5</span>
)</code></pre></div>
<pre><code>## [1] &quot;desigining treatments Wed Oct  7 10:14:54 2015&quot;
## [1] &quot;design var x Wed Oct  7 10:14:54 2015&quot;
## [1] &quot;scoring treatments Wed Oct  7 10:14:55 2015&quot;
## [1] &quot;have treatment plan Wed Oct  7 10:14:56 2015&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dTrainTreated &lt;-<span class="st"> </span>vtreat::<span class="kw">prepare</span>(treatments,dTrain,
                                 <span class="dt">pruneSig=</span><span class="kw">c</span>() <span class="co"># Note: set this to filter, like 0.05 or 1/nvars</span>
)
m2 &lt;-<span class="st"> </span><span class="kw">glm</span>(y~x_catB,<span class="dt">data=</span>dTrainTreated,<span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">'logit'</span>))
<span class="kw">print</span>(<span class="kw">summary</span>(m2)) <span class="co"># notice high residual deviance</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ x_catB, family = binomial(link = &quot;logit&quot;), 
##     data = dTrainTreated)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.181  -1.181   1.173   1.174   1.174  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)  0.0080649  0.0624150   0.129    0.897
## x_catB      -0.0001655  0.0225930  -0.007    0.994
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1681.6  on 1212  degrees of freedom
## Residual deviance: 1681.6  on 1211  degrees of freedom
## AIC: 1685.6
## 
## Number of Fisher Scoring iterations: 3</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dTrain$predM2 &lt;-<span class="st"> </span><span class="kw">predict</span>(m2,<span class="dt">newdata=</span>dTrainTreated,<span class="dt">type=</span><span class="st">'response'</span>)
<span class="kw">plotRes</span>(dTrain,<span class="st">'predM2'</span>,<span class="st">'y'</span>,<span class="st">'model2 on train'</span>)</code></pre></div>
<pre><code>## [1] &quot;model2 on train&quot;
##        pred
## truth   TRUE
##   FALSE  604
##   TRUE   609
## [1] &quot;accuracy 0.497938994229184&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># We do not advise creating dCodeTreated for any purpose other than</span>
<span class="co"># diagnostic plotting.  You should not use the treated coding data</span>
<span class="co"># for anything (as that would undo the benefit of having a separate</span>
<span class="co"># coding data subset).</span>
dCodeTreated &lt;-<span class="st"> </span>vtreat::<span class="kw">prepare</span>(treatments,dCode,<span class="dt">pruneSig=</span><span class="kw">c</span>())
dCode$predM2 &lt;-<span class="st"> </span><span class="kw">predict</span>(m2,<span class="dt">newdata=</span>dCodeTreated,<span class="dt">type=</span><span class="st">'response'</span>)
<span class="kw">plotRes</span>(dCode,<span class="st">'predM2'</span>,<span class="st">'y'</span>,<span class="st">'model2 on coding set'</span>)</code></pre></div>
<pre><code>## [1] &quot;model2 on coding set&quot;
##        pred
## truth   TRUE
##   FALSE  204
##   TRUE   221
## [1] &quot;accuracy 0.48&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dTestTreated &lt;-<span class="st"> </span>vtreat::<span class="kw">prepare</span>(treatments,dTest,<span class="dt">pruneSig=</span><span class="kw">c</span>())
dTest$predM2 &lt;-<span class="st"> </span><span class="kw">predict</span>(m2,<span class="dt">newdata=</span>dTestTreated,<span class="dt">type=</span><span class="st">'response'</span>)
<span class="kw">plotRes</span>(dTest,<span class="st">'predM2'</span>,<span class="st">'y'</span>,<span class="st">'model2 on test set'</span>)</code></pre></div>
<pre><code>## [1] &quot;model2 on test set&quot;
##        pred
## truth   TRUE
##   FALSE  175
##   TRUE   187
## [1] &quot;accuracy 0.483425414364641&quot;</code></pre>
<p>In the above example we saw training and test performance are similar (and where they should be as there is no signal). Notice the coding set (falsely) shows high performance. This is the bad behavior we wanted to isolate out of the training set.</p>
<p>Also be wary, on small data sets vtreat::designTreatments can not always get accurate out-of sample estimates of variable performance (in these cases it falls back to untrustworthy in-sample estimates). This is something we will improve over time, but vtreat is intended mostly for production applications on large data sets.</p>
<p>Bad small example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">treatmentsBad &lt;-<span class="st"> </span>vtreat::<span class="kw">designTreatmentsC</span>(d[d$rgroup&lt;=<span class="dv">0</span>,,<span class="dt">drop=</span><span class="ot">FALSE</span>],<span class="st">'x'</span>,<span class="st">'y'</span>,<span class="ot">TRUE</span>,
                      <span class="dt">rareCount=</span><span class="dv">0</span> <span class="co"># Note set this to something larger, like 5</span>
)</code></pre></div>
<pre><code>## [1] &quot;desigining treatments Wed Oct  7 10:14:56 2015&quot;
## [1] &quot;design var x Wed Oct  7 10:14:56 2015&quot;
## [1] &quot;scoring treatments Wed Oct  7 10:14:56 2015&quot;
## [1] &quot;have treatment plan Wed Oct  7 10:14:57 2015&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(treatmentsBad$scoreFrame[treatmentsBad$scoreFrame$sig&lt;=<span class="fl">0.05</span>,,<span class="dt">drop=</span><span class="ot">FALSE</span>])</code></pre></div>
<pre><code>## [1] varName       origName      varMoves      PRESSRsquared psig         
## [6] sig           catPRSquared  csig         
## &lt;0 rows&gt; (or 0-length row.names)</code></pre>
<p>We would have preferred no variables to have received a “good score.”</p>
<p>So even better is to perform the 3-way data split, set pruneSig to something reasonable, and do not set rareCount to zero (leave it at the default or a reasonable count like 5 or 10).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dCode &lt;-<span class="st"> </span>d[d$rgroup&lt;=<span class="dv">20</span>,,drop=<span class="ot">FALSE</span>]
dTrain &lt;-<span class="st"> </span>d[(d$rgroup&gt;<span class="dv">20</span>) &amp;<span class="st"> </span>(d$rgroup&lt;=<span class="dv">80</span>),,drop=<span class="ot">FALSE</span>]
<span class="kw">tryCatch</span>(
  { treatments &lt;-<span class="st"> </span>vtreat::<span class="kw">designTreatmentsC</span>(dCode,<span class="st">'x'</span>,<span class="st">'y'</span>,<span class="ot">TRUE</span>)
  dTrainTreated &lt;-<span class="st"> </span>vtreat::<span class="kw">prepare</span>(treatments,dTrain,<span class="dt">pruneSig=</span><span class="fl">0.01</span>)},
  <span class="dt">error=</span>function(x) <span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">'caught'</span>,x))
)</code></pre></div>
<pre><code>## [1] &quot;desigining treatments Wed Oct  7 10:14:57 2015&quot;
## [1] &quot;design var x Wed Oct  7 10:14:57 2015&quot;
## [1] &quot;scoring treatments Wed Oct  7 10:14:57 2015&quot;
## [1] &quot;have treatment plan Wed Oct  7 10:14:57 2015&quot;
## [1] &quot;caught Error in vtreat::prepare(treatments, dTrain, pruneSig = 0.01): no usable vars\n&quot;</code></pre>
<p>And in this case we are (properly) told are no variables to work with (and prevented from accidentally continuing a bad analysis).</p>
<p>There are, of course, other methods to avoid the bias introduced in using the same data to generate the variable encodings and then train a model using those variables. vtreat incorporates a number of these (smoothing controlled through ‘smFactor’, pruning of rare levels controlled through ‘rareSig’ and ‘rareCount’, and <a href="http://www.win-vector.com/blog/2015/10/using-differential-privacy-to-reuse-training-data/">cross-constructed training frames</a> accessed by setting ‘returnXFrame=TRUE’). But we feel when you have a lot of data the simplicity (and statistical soundness) of the three way split is attractive.</p>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
