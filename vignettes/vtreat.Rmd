---
title: "vtreat"
author: "John Mount, Nina Zumel"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

vtreat is a package that prepares arbitrary data frames into
clean data frames that are ready for analysis.  A clean data
frame:

- Only has numeric columns (other than the outcome).
- Has no Infinite/NA/NaN in the effective variable columns.


To achieve this a number of techniques are used.  Principally:

- [Impact coding](http://www.win-vector.com/blog/2012/07/modeling-trick-impact-coding-of-categorical-variables-with-many-levels/)
- [Encoding category levels as indicators](http://www.win-vector.com/blog/2014/12/a-comment-on-preparing-data-for-classifiers/)

For more details see: [the vtreat article](http://www.win-vector.com/blog/2014/08/vtreat-designing-a-package-for-variable-treatment/)

The main pattern is the use of designTreatmentsC() or designTreatmentsN() to design a treatment plan and then use the returned structure with prepare() to apply the plan to data frames.  The main feature of vtreat is all data preparation is "y-aware" or uses the relations of effective variables to the dependent or outcome variable to encode the effective variables.

The structure returned from designTreatmentsN() or designTreatmentsC() includes informational fields.  The main fields are mostly vectors with names (all with the same names in the same order):

- vars : (character array without names) names of variables (in same order as names on the other diagnostic vectors)
- varMoves : logical TRUE if the variable varied during training, only variables that move will be in the treated frame.
- sig : significance of observed variable predictive value under an in-sample permutation test.

In addition to these vectors designTreatmentsC() and designTreatmentsN() return a data frame named scoreFrame which contains columns:
- varName: name of new variable
- origName: name of original variable variable was derived from (can repeat)
- varMoves : logical TRUE if the variable varied during training, only variables that move will be in the treated frame.
- PRESSRsquared : a PRESS-held out R-squared of a linear fit from each variable to the y-value.  Scores of zero and below are very bad, scores near one are very good.
- psig : significance of observed variable PRESSRsquared value under an in-sample permutation test.
- catPRSquared : for categorical outcomes: deviance based pseudo-Rsquared.
- csig : for categorical outcomes: significance of observed variable catPRSquared value under an in-sample permutation test.
- sig : csig for categorical outcomes, psig otherwise.


In all cases we have two upward biases on the scores

- The treated variables view the training data during construction (for setting of NA values, missing values, levels, and more).  So this gives an upward bias when trying to measure treated variable utility on training data.  Until the data set is at least 1000 good rows we ignore this effect.  After 1000 rows we design variables on a pseudo-random chosen 80% of the rows and score on the complimentary 20% of the rows.
- The scoring procedure itself involves a fit (linear regression for PRESSRsquared or logistic regression for catPseudoRSquared).  So in each of these cases we would like the regression itself to be only evaluated on held-out data.  The PRESS statistic does just that (fast 1-way cross validation).   The catPseudoRSquared performs explicit 1-way cross validation for small data sets and hold-out scoring for larger data sets.

The suggested best practice is (if you have enough data) to split your randomly into at least the following disjoint data sets:

- Encoding Calibration : a data set used for the designTreatmentsC() or designTreatmentsN() step and not used again for training or test.
- Training : a data set used (after prepare()) for training your model.
- Test : a data set used (after prepare()) for estimating your model's out of training performance.

The idea is: taking the extra step to perform the designTreatmentsC() or designTreatmentsN() on data disjoint from training makes the training data more exchangable with test and avoids the issue that vtreat may be hiding a large number of degrees of freedom in variables it derives from large categoricals.

An trivial execution example (not demonstrating any cal/train/test split) is given below.  Varaibles that do not move during hold-out testing are considred "not to move."

```{r, tidy=FALSE}
library(vtreat)
dTrainC <- data.frame(x=c('a','a','a','b','b',NA),
   z=c(1,2,3,4,NA,6),y=c(FALSE,FALSE,TRUE,FALSE,TRUE,TRUE))
head(dTrainC)

dTestC <- data.frame(x=c('a','b','c',NA),z=c(10,20,30,NA))
head(dTestC)

treatmentsC <- designTreatmentsC(dTrainC,colnames(dTrainC),'y',TRUE)
print(treatmentsC)
print(treatmentsC$treatments[[1]])

dTrainCTreated <- prepare(treatmentsC,dTrainC,pruneSig=c(),scale=TRUE)
head(dTrainCTreated)

varsC <- setdiff(colnames(dTrainCTreated),'y')
# all input variables should be mean 0
sapply(dTrainCTreated[,varsC,drop=FALSE],mean)
# all slopes should be 1
sapply(varsC,function(c) { lm(paste('y',c,sep='~'),
   data=dTrainCTreated)$coefficients[[2]]})

dTestCTreated <- prepare(treatmentsC,dTestC,pruneSig=c(),scale=TRUE)
head(dTestCTreated)


# numeric example
dTrainN <- data.frame(x=c('a','a','a','a','b','b',NA),
   z=c(1,2,3,4,5,NA,7),y=c(0,0,0,1,0,1,1))
head(dTrainN)

dTestN <- data.frame(x=c('a','b','c',NA),z=c(10,20,30,NA))
head(dTestN)

treatmentsN = designTreatmentsN(dTrainN,colnames(dTrainN),'y')
print(treatmentsN)

dTrainNTreated <- prepare(treatmentsN,dTrainN,
                          pruneSig=c(),scale=TRUE)
head(dTrainNTreated)

varsN <- setdiff(colnames(dTrainNTreated),'y')
# all input variables should be mean 0
sapply(dTrainNTreated[,varsN,drop=FALSE],mean) 
# all slopes should be 1
sapply(varsN,function(c) { lm(paste('y',c,sep='~'),
   data=dTrainNTreated)$coefficients[[2]]}) 
dTestNTreated <- prepare(treatmentsN,dTestN,
                         pruneSig=c(),scale=TRUE)
head(dTestNTreated)
```